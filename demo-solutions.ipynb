{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an RNN, from scratch!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote my own RNN from scratch based on [Ryan's nice slides](https://github.com/alan-turing-institute/transformers-reading-group/blob/main/sessions/03-seq2seq-part-i/seq2seq_part1_hut23_robots_in_disguise.pdf), and this notebook is the result. Now, I'll guide you to do the same (but using a lot of my boilerplate, so you can just do the fun stuff!)\n",
    "\n",
    "This notebook assumes familiarity with `numpy` and how to multiply arrays. The reason is that we're gonna use my favourite deep learing library [`jax`](https://github.com/google/jax), which uses the same syntax as `numpy`, but we just `import jax.numpy as jnp` instead! Everything else is more or less the same :)\n",
    "\n",
    "The basic layout is as follows: I'm gonna show you the formula, and write the function signature with some type annotations as hints. You're gonna fill in the blanks! But don't panic: most of these functions are one or two-liners that I literally just copied from the slides.\n",
    "\n",
    "In these functions, you'll see type annotations like this a lot:\n",
    "```python3\n",
    "x: Float[Array, \"dim1 dim2\"]\n",
    "```\n",
    "What am I meaning here? This just says that we have a variable `x`, which is an array of floating-point values (hence the `Float`). The string `\"dim1 dim2\"` is syntax for the shape of the array, which would have two dimensions `dim1` and `dim2`, i.e. a matrix with `dim1` rows and `dim2` columns. They don't represent anything concrete until we actually instantiate the value of `x`, but paying attention to these dimensions will help you make sure your matrix-vector multiplications will work (remember the rules of matrix multiplication: this matrix could only multiply an object on the right with leading dimension `dim2`). Oh, and if you're wondering where this syntax comes from -- it's the library [`jaxtyping`](https://github.com/google/jaxtyping)!\n",
    "\n",
    "Speaking of which, let's install a couple dependencies first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install equinox jaxtyping jaxlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows below is a bit messier -- this is a boilerplate function designed to pre-process a text document and form a vocabulary. This is normally handled by external libraries, but I was going all-in on doing this from scratch :p\n",
    "\n",
    "Don't try too hard to read it -- we will play with the output in the next step!\n",
    "\n",
    "The basic gist:\n",
    "- Find all unique tokens (including newlines, punctuation, contracted words like it's)\n",
    "- Create a unique identifier for each token -- it's position in a list of the vocabulary\n",
    "- Map the text to its corresponding indicies in the vocabulary\n",
    "- Split this up into a training set (what the model sees) and a validation set (what it doesn't see) so we can evaluate our generalisation capabilities to unseen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import re\n",
    "from jaxtyping import Array, Float, Int\n",
    "import pytreeclass as pytc\n",
    "from typing import Generator\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def prepare_text(file_name, sentence_length):\n",
    "    with open(file_name, \"r+\") as file:\n",
    "        all_text = file.read()\n",
    "        # all_text = all_text.replace('\\n', ' ').replace('  : ', '')\n",
    "\n",
    "    # Define a regular expression pattern to match all punctuation marks\n",
    "    punctuation_pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "    # Define a regular expression pattern to match words with apostrophes\n",
    "    apostrophe_pattern = r\"\\w+(?:\\'\\w+)?\"\n",
    "    # Define a regular expression pattern to match newlines\n",
    "    newline_pattern = r\"\\n\"\n",
    "\n",
    "    # Combine the three patterns to match all tokens\n",
    "    token_pattern = (\n",
    "        punctuation_pattern + \"|\" + apostrophe_pattern + \"|\" + newline_pattern\n",
    "    )\n",
    "\n",
    "    # Split the text into tokens, including words with apostrophes as separate tokens\n",
    "    all_words = re.findall(token_pattern, all_text.lower())\n",
    "    vocab = list(set(all_words))\n",
    "\n",
    "    vocab_one_hot_indicies = jnp.array(\n",
    "        [vocab.index(t) for t in all_words], dtype=jnp.int32\n",
    "    )\n",
    "    split_indicies = vocab_one_hot_indicies[\n",
    "        : (len(vocab) // sentence_length) * sentence_length\n",
    "    ].reshape(len(vocab) // sentence_length, sentence_length)\n",
    "    # make last word random, shouldn't make too much of an impact (could be better handled with special char?)\n",
    "    split_indicies_labels = jnp.concatenate(\n",
    "        (\n",
    "            vocab_one_hot_indicies[\n",
    "                1 : ((len(vocab) - 1) // sentence_length) * sentence_length\n",
    "            ],\n",
    "            jnp.array([0]),\n",
    "        )\n",
    "    ).reshape((len(vocab) - 1) // sentence_length, sentence_length)\n",
    "    partition_index = 6 * int(len(split_indicies) / 7)\n",
    "    train = split_indicies[:partition_index]\n",
    "    train_labels = split_indicies_labels[:partition_index]\n",
    "    valid = split_indicies[partition_index:]\n",
    "    valid_labels = split_indicies_labels[partition_index:]\n",
    "\n",
    "    return train, train_labels, valid, valid_labels, vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producing a vocabulary from text\n",
    "\n",
    "Below, we explore the data we're working with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples from vocab: ['thin', 'brush', 'one', 'may', 'called', 'sad', 'look', 'but', 'high', 'get']\n",
      "total length of vocab: 300 unique words\n",
      "total length of training data: 30 sentences (each 8 words)\n",
      "total length of validation data: 7 sentences (each 8 words)\n"
     ]
    }
   ],
   "source": [
    "file_name = \"one-fish-two-fish.txt\"\n",
    "sentence_length = 8  # keep even because of how we split the data\n",
    "train, train_labels, valid, valid_labels, vocab = prepare_text(\n",
    "    file_name, sentence_length\n",
    ")\n",
    "\n",
    "print(f\"examples from vocab: {vocab[:10]}\")\n",
    "print(f\"total length of vocab: {len(vocab)} unique words\")\n",
    "print(\n",
    "    f\"total length of training data: {len(train)} sentences (each {sentence_length} words)\"\n",
    ")\n",
    "print(\n",
    "    f\"total length of validation data: {len(valid)} sentences (each {sentence_length} words)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([  2, 186, 233, 227, 186, 233,  83, 186], dtype=int32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first sentence in train set\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([186, 233, 227, 186, 233,  83, 186, 233], dtype=int32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first sentence in train labels == same sentence shifted by one word\n",
    "# i.e. equivalent to train[0][1:] + train[1][0]\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one fish , two fish , red fish'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can reconstruct a sentence by mapping indicies back to words\n",
    "\" \".join([vocab[i] for i in train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fish , two fish , red fish ,'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([vocab[i] for i in train_labels[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our whole text has been split into many individual words, or *tokens*. Of course, we don't have to model words; we can use characters, strings of length 4, numbers... anything goes! For simplicity, we assume one word <-> one token here. Then, I'm thinking of a vocabulary as a lookup table that maps a word seen in the text to a unique numerical identifier -- this can just be the position of that word in the vocabulary, assuming it's a list-like structure. We're including things like punctuation, contracted words, newlines etc.\n",
    "\n",
    "What next? For our RNN, we need to be able to take in a sentence -- assumed to be a list of indicies of positions in the vocabulary -- and construct one-hot vector for each. Recall the definition of a one-hot vector here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish_idx = vocab.index(\"fish\")  # the index of the word \"fish\" in the vocab\n",
    "one_hot_fish = np.zeros(len(vocab))  # a vector of zeros with length equal to the vocab\n",
    "one_hot_fish[fish_idx] = 1  # set the index of the word \"fish\" to 1\n",
    "\n",
    "# the syntax in JAX is a little different, but the idea is the same\n",
    "# we use the `at` method to set the value at a particular index,\n",
    "# and the `set` method to set the value at that index to 1.\n",
    "# this is due to the fact that JAX arrays are immutable,\n",
    "# so we can't just set the value at an index to 1 directly!\n",
    "fish_idx = vocab.index(\"fish\")\n",
    "one_hot_fish = jnp.zeros(len(vocab))\n",
    "one_hot_fish = one_hot_fish.at[fish_idx].set(1)\n",
    "\n",
    "one_hot_fish"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above, fill in the function to turn a sentence of indicies -- corresponding to words in the vocab -- into an array of one-hot vectors for those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_sentence(\n",
    "    sentence: Int[Array, \"sentence\"], vocab_size: int\n",
    ") -> Int[Array, \"sentence vocab\"]:\n",
    "    return jnp.array([jnp.zeros((vocab_size,)).at[word].set(1) for word in sentence])\n",
    "\n",
    "\n",
    "# make a very intelligent sentence of all \"fish\"\n",
    "fish_sentence = jnp.array([fish_idx] * 10)\n",
    "\n",
    "# one hot encode the sentence\n",
    "one_hot_fish_sentence = one_hot_sentence(fish_sentence, len(vocab))\n",
    "\n",
    "# assert that the sentence is one hot encoded correctly\n",
    "assert jnp.all(one_hot_fish_sentence == jnp.array([one_hot_fish] * 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use `vmap` to automatically transform the function to work on a batch of sentences!\n",
    "# this will be useful when we want to train our model on multiple sentences at once.\n",
    "# note that we need to specify the `in_axes` argument to tell JAX which argument\n",
    "# in the function is the one that we want to map over (in this case, we want to\n",
    "# map over the first axis of the `sentence` argument, indicated by `0`).\n",
    "# we also need to specify `None` for the `vocab_size` argument, since it is not\n",
    "# being mapped over -- it is the same for every sentence in the batch.\n",
    "batch_one_hot = jax.vmap(one_hot_sentence, in_axes=(0, None))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "What's an embedding? In this context, its a reduction in dimensionality on the vocabulary that should contain some useful information about language. Usually, people use [pre-trained embeddings](https://huggingface.co/blog/getting-started-with-embeddings) for tasks like text generation (since language is a fairly general setting), but we're taking the ambitious route of learning the embedding matrix jointly with the weights. It's likely that the results of this notebook would be much better if we used embeddings that already encoded information about language and the relationship between words (though, the vocabulary would then need to be much larger than what we use later, which just finds all the unique words in a text document).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to take an aside to define our embedding matrix and other relevant quantities. Let's look at the parameters of our RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────────────┬───────────┬──────┐\n",
      "│Name                 │Type       │Count │\n",
      "├─────────────────────┼───────────┼──────┤\n",
      "│.embedding_weights   │f32[16,30] │480   │\n",
      "├─────────────────────┼───────────┼──────┤\n",
      "│.hidden_state_weights│f32[16,16] │256   │\n",
      "├─────────────────────┼───────────┼──────┤\n",
      "│.output_weights      │f32[300,16]│4,800 │\n",
      "├─────────────────────┼───────────┼──────┤\n",
      "│.hidden_state_bias   │f32[16]    │16    │\n",
      "├─────────────────────┼───────────┼──────┤\n",
      "│.output_bias         │f32[300]   │300   │\n",
      "├─────────────────────┼───────────┼──────┤\n",
      "│.embedding_matrix    │f32[30,300]│9,000 │\n",
      "├─────────────────────┼───────────┼──────┤\n",
      "│Σ                    │Parameters │14,852│\n",
      "└─────────────────────┴───────────┴──────┘\n",
      "Parameters\n",
      "├── .embedding_weights=f32[16,30](μ=-0.00, σ=0.06, ∈[-0.10,0.10])\n",
      "├── .hidden_state_weights=f32[16,16](μ=0.06, σ=0.24, ∈[0.00,1.00])\n",
      "├── .output_weights=f32[300,16](μ=-0.00, σ=0.06, ∈[-0.10,0.10])\n",
      "├── .hidden_state_bias=f32[16](μ=0.00, σ=0.00, ∈[0.00,0.00])\n",
      "├── .output_bias=f32[300](μ=0.00, σ=0.00, ∈[0.00,0.00])\n",
      "└── .embedding_matrix=f32[30,300](μ=-0.00, σ=0.06, ∈[-0.10,0.10])\n"
     ]
    }
   ],
   "source": [
    "# This is a container for all the free parameters of an RNN!\n",
    "# You can see the shapes of each attribute from the type annotations.\n",
    "# We have a couple of sizes: hidden_state, embedding, vocab\n",
    "# -> these represent the size of the hidden state weights,\n",
    "#    the embedding matrix, and the vocabulary respectively.\n",
    "# This parameters object will be passed to most functions below:\n",
    "# e.g. access the output weights by calling `params.output_weights` etc.\n",
    "class Parameters(pytc.TreeClass):\n",
    "    embedding_weights: Float[Array, \"hidden_state embedding\"]\n",
    "    hidden_state_weights: Float[Array, \"hidden_state hidden_state\"]\n",
    "    output_weights: Float[Array, \"vocab hidden_state\"]\n",
    "    hidden_state_bias: Float[Array, \"hidden_state\"]\n",
    "    output_bias: Float[Array, \"vocab\"]\n",
    "    embedding_matrix: Float[Array, \"embedding vocab\"]\n",
    "\n",
    "\n",
    "# we'll initialize our parameters randomly, but close to 0/identity so that\n",
    "# we don't have exploding gradients later on!\n",
    "\n",
    "# set sizes for embeddings, hidden state, vocab, and output vectors\n",
    "e = 30\n",
    "h = 16\n",
    "v = len(vocab)\n",
    "o = v\n",
    "\n",
    "params = Parameters(\n",
    "    embedding_weights=jax.random.truncated_normal(\n",
    "        lower=-0.1, upper=0.1, shape=[h, e], key=jax.random.PRNGKey(0)\n",
    "    ),\n",
    "    hidden_state_weights=jnp.identity(h),\n",
    "    output_weights=jax.random.truncated_normal(\n",
    "        lower=-0.1, upper=0.1, shape=[o, h], key=jax.random.PRNGKey(0)\n",
    "    ),\n",
    "    hidden_state_bias=jnp.zeros((h,)),\n",
    "    output_bias=jnp.zeros(\n",
    "        shape=[\n",
    "            o,\n",
    "        ]\n",
    "    ),\n",
    "    embedding_matrix=jax.random.truncated_normal(\n",
    "        lower=-0.1, upper=0.1, shape=[e, v], key=jax.random.PRNGKey(0)\n",
    "    ),\n",
    ")\n",
    "\n",
    "# let's inspect the structure of our parameters\n",
    "print(pytc.tree_summary(params))\n",
    "print(pytc.tree_diagram(params))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll come to use most of these values later, but for now, we're just focused on embeddings!\n",
    "\n",
    "Recall what we did in the previous sessions, looking at RNNs for *language modelling*, where every word is turned into a one-hot vector (or \"token\"). We then multiply these one-hot words by an *embedding matrix* $E$, which multiplies the words to reduce the dimension of that long one-hot vector (=size of the whole vocabulary) to some specified lower dimensional representation (normally ~100 ish). This embedded word is then used to update the hidden state $h$ of the RNN.\n",
    "\n",
    "Use the embedding matrix (accessible through `params.embedding_matrix`) to fill in the function below, which embeds a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings(\n",
    "    one_hot_word: Float[Array, \"vocab\"], params: Parameters\n",
    ") -> Float[Array, \"embedding\"]:\n",
    "    return params.embedding_matrix @ one_hot_word\n",
    "\n",
    "\n",
    "# I should be a vector of length `e`\n",
    "assert make_embeddings(one_hot_fish, params).shape == (e,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map to work over sentences for later!\n",
    "embeddings_map = jax.vmap(make_embeddings, in_axes=(0, None))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the hidden state\n",
    "\n",
    "Assume some hidden state $h^{(t-1)}$, and we're trying to go to hidden state $h^{(t)}$ given our next embedded word, and the set of free parameters. Using the slides, fill in this function -- for $\\sigma$, you can use the activation function `jax.nn.tanh`. Here's the slide as a reminder:\n",
    "\n",
    "![](images/main_rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_hidden_state(\n",
    "    embedding: Float[Array, \"embedding\"],\n",
    "    hidden_state: Float[Array, \"hidden_state\"],\n",
    "    params: Parameters,\n",
    ") -> Float[Array, \"hidden_state\"]:\n",
    "    return jax.nn.tanh(\n",
    "        params.hidden_state_weights @ hidden_state\n",
    "        + params.embedding_weights @ embedding\n",
    "        + params.hidden_state_bias\n",
    "    )\n",
    "\n",
    "\n",
    "# I should be a vector of length `h`\n",
    "embedding = make_embeddings(one_hot_fish, params)\n",
    "assert update_hidden_state(embedding, jnp.zeros((h,)), params).shape == (h,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing an output\n",
    "\n",
    "Once we've updated the hidden state, we're able to produce an output from this timestep! Remember that this approach to language modelling means defining a probability distribution across our vocabulary -- this means that for each word, we assign a number that represents how likely that word is to appear next; our output $\\mathbf{\\hat{y}^{(t)}}$ is then a vector of the same length as the vocabulary with a set of numbers in the range 0-1 assigned to each word.\n",
    "\n",
    "Using this knowledge, and referring to the slide, implement the output computation using the current hidden state $h^{(t)}$ and the RNN parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(\n",
    "    hidden_state: Float[Array, \"hidden_state\"], params: Parameters\n",
    ") -> Float[Array, \"vocab\"]:\n",
    "    return jax.nn.softmax(params.output_weights @ hidden_state + params.output_bias)\n",
    "\n",
    "\n",
    "# I should be a vector of length `v`\n",
    "assert output(jnp.zeros((h,)), params).shape == (v,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Now all that's left to produce output from the RNN is to compose the functions from above!\n",
    "\n",
    "I've pseudocoded the function already using comments -- see if you can fill it in. Note that the input to the RNN here is a sentence (i.e. a sequence of one-hot-encoded words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['then', 'then', 'then', 'then', 'then', 'then', 'then', 'then', 'sun', 'sun']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rnn(\n",
    "    data: Float[Array, \"sentence vocab\"], params: Parameters, hidden_size: int\n",
    ") -> Float[Array, \"sentence vocab\"]:\n",
    "    # apply embeddings_map to create a vector of embeddings\n",
    "    embeddings = embeddings_map(data, params)  # [\"sentence embedding\"]\n",
    "\n",
    "    # initialize the hidden state with zeros\n",
    "    hidden_state = jnp.zeros((hidden_size,))\n",
    "\n",
    "    # for each word in the vector of embeddings:\n",
    "    #   > update the hidden state\n",
    "    #   > compute the output word using that hidden state and store it\n",
    "    # return the set of outputs\n",
    "    outputs = []\n",
    "\n",
    "    for word in embeddings:\n",
    "        hidden_state = update_hidden_state(word, hidden_state, params)\n",
    "        outputs.append(output(hidden_state, params))\n",
    "\n",
    "    return jnp.array(outputs)\n",
    "\n",
    "\n",
    "# make a very intelligent sentence of all \"fish\"\n",
    "fish_sentence = jnp.array([fish_idx] * 10)\n",
    "one_hot_fish_sentence = one_hot_sentence(fish_sentence, len(vocab))\n",
    "\n",
    "# run the RNN on \"fish fish fish fish fish fish fish fish fish fish\"\n",
    "rnn_outputs = rnn(one_hot_fish_sentence, params, h)\n",
    "\n",
    "# the output should be a list of 10 vectors of length `v`\n",
    "# corresponding to the output probabilities at each position in the sentence\n",
    "assert rnn_outputs.shape == (10, v)\n",
    "\n",
    "# what is the most likely word at each position?\n",
    "# (remember that we have randomly initialized our parameters, so this will be nonsense!)\n",
    "most_likely_words = [vocab[jnp.argmax(output)] for output in rnn_outputs]\n",
    "most_likely_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching the network through the loss (teacher forcing)\n",
    "\n",
    "What's a good metric to see if the language modelling is working correctly? Well, if we're predicting the next word in the sentence, and we happen to have access to the whole sentence, we can just see what probability the model assigns to the correct next word. Since that's something we want to maximise, but neural networks are usually trained to minimise the objective, we can do the common trick of taking the negative log of that quantity to serve as our \"loss\" function -- the thing that we expect to be small when we're doing well. Using this and the slide below, implement the loss function for our RNN.\n",
    "\n",
    "Note that you only need to do this for a single output word -- we'll use the function `jax.vmap` to automatically vectorise the function to handle whole sentences!\n",
    "\n",
    "![](images/rnn_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(\n",
    "    output: Float[Array, \"vocab\"], next_one_hot_word: Float[Array, \"vocab\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    # index the softmax probs at the word of interest\n",
    "    return -jnp.log(output[jnp.argmax(next_one_hot_word)])\n",
    "\n",
    "\n",
    "sentence_loss = jax.vmap(loss, in_axes=(0, 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a loss function we can compute over sentences, we can make a pipeline that goes from initial data all the way to the loss function result. The reason we need to be explicit about this is the following: in order to calculate the gradient of the loss function in `jax`, we need to have a function that takes in the thing we want to differentiate with respect to (here, the `Parameters`), and returns the loss result, which can only be computed *after* the model has been run.\n",
    "\n",
    "You can see this below: note that we're taking the mean of the loss over the sentence -- this means that the RNN is targeting the goal of getting the correct word on average across the sentences we feed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loss: 5.69\n",
      "gradients are packed up in a Parameters object:\n",
      "Parameters\n",
      "├── .embedding_weights=f32[16,30](μ=-0.00, σ=0.02, ∈[-0.05,0.05])\n",
      "├── .hidden_state_weights=f32[16,16](μ=0.00, σ=0.02, ∈[-0.04,0.06])\n",
      "├── .output_weights=f32[300,16](μ=-0.00, σ=0.01, ∈[-0.15,0.20])\n",
      "├── .hidden_state_bias=f32[16](μ=-0.11, σ=0.28, ∈[-0.50,0.32])\n",
      "├── .output_bias=f32[300](μ=0.00, σ=0.06, ∈[-1.00,0.00])\n",
      "└── .embedding_matrix=f32[30,300](μ=-0.00, σ=0.00, ∈[-0.15,0.07])\n"
     ]
    }
   ],
   "source": [
    "def forward_pass(\n",
    "    data: Float[Array, \"sentence vocab\"],\n",
    "    next_words: Float[Array, \"sentence vocab\"],  # data shifted by 1 to the right\n",
    "    params: Parameters,\n",
    "    hidden_size: int,\n",
    ") -> Float[Array, \"\"]:\n",
    "    output = rnn(data, params, hidden_size)\n",
    "    return sentence_loss(output, next_words).mean(axis=0)\n",
    "\n",
    "\n",
    "# run the forward pass on our fish sentence\n",
    "loss_value = forward_pass(one_hot_fish_sentence, one_hot_fish_sentence, params, h)\n",
    "assert loss_value.shape == ()\n",
    "print(f\"starting loss: {loss_value:.2f}\")\n",
    "\n",
    "# here, we transform the forward pass into the gradient function,\n",
    "# and also vmap again so it can handle a batch of sentences instead of one.\n",
    "loss_and_gradient = jax.value_and_grad(forward_pass, argnums=2)\n",
    "print(\"gradients are packed up in a Parameters object:\")\n",
    "print(\n",
    "    pytc.tree_diagram(\n",
    "        loss_and_gradient(one_hot_fish_sentence, one_hot_fish_sentence, params, h)[1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also vmap the gradient function to handle a batch of sentences,\n",
    "# and jit it to make it faster!\n",
    "batched_grads = jax.jit(\n",
    "    jax.vmap(loss_and_gradient, in_axes=(0, 0, None, None)), static_argnums=(3,)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've done all the hard work! We'll not write any more code -- it's all pretty generic boilerplate for training any ML model.\n",
    "\n",
    "### Predicting words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another boilerplate-y function: turning the softmaxed probabilities into actual words. The only real thing going on here is turning a softmax into a hard-max using `jnp.argmax` -- then we can just convert back from the one-hot representation. One other thing is that we can technically predict arbitrarily many words by putting every prediction from the RNN as an input to generate new hidden state and new output. You'll find this cycle to very quickly produce some funny-looking sentences if you let it go on for too many tokens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_words(\n",
    "    prompt: str,\n",
    "    vocab: list[str],\n",
    "    rnn_params: Parameters,\n",
    "    rnn_hidden_size: int,\n",
    "    num_predicted_tokens: int,\n",
    "    include_prompt=True,\n",
    ") -> str:\n",
    "    # Define a regular expression pattern to match all punctuation marks\n",
    "    punctuation_pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "    # Define a regular expression pattern to match words with apostrophes\n",
    "    apostrophe_pattern = r\"\\w+(?:\\'\\w+)?\"\n",
    "    # Define a regular expression pattern to match newlines\n",
    "    newline_pattern = r\"\\n\"\n",
    "\n",
    "    # Combine the three patterns to match all tokens\n",
    "    token_pattern = (\n",
    "        punctuation_pattern + \"|\" + apostrophe_pattern + \"|\" + newline_pattern\n",
    "    )\n",
    "\n",
    "    tokens = re.findall(token_pattern, prompt.lower())\n",
    "    one_hot_indicies = jnp.array([vocab.index(t) for t in tokens], dtype=jnp.int32)\n",
    "    sentence = one_hot_sentence(one_hot_indicies, len(vocab))\n",
    "    embeddings = embeddings_map(sentence, rnn_params)  # [\"sentence embedding\"]\n",
    "\n",
    "    hidden_state = jnp.zeros((rnn_hidden_size,))\n",
    "    outputs = [None] * num_predicted_tokens\n",
    "    for word in embeddings[:-1]:\n",
    "        hidden_state = update_hidden_state(word, hidden_state, rnn_params)\n",
    "    hidden_state = update_hidden_state(embeddings[-1], hidden_state, rnn_params)\n",
    "    outputs[0] = output(hidden_state, rnn_params)\n",
    "\n",
    "    for i in range(1, num_predicted_tokens):\n",
    "        embedded_pred = make_embeddings(outputs[i - 1], rnn_params)\n",
    "        hidden_state = update_hidden_state(embedded_pred, hidden_state, rnn_params)\n",
    "        outputs[i] = output(hidden_state, rnn_params)\n",
    "\n",
    "    res = jnp.array(outputs)\n",
    "    res_indicies = jnp.argmax(res, axis=1)\n",
    "    words = [vocab[i] for i in res_indicies]\n",
    "    out = \" \".join(words)\n",
    "    return prompt + \" | \" + out if include_prompt else out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "We'll go through some more code to set up batching, initialize our parameters, and look at the training loop. You don't need to write any more code -- it should all just work now!\n",
    "\n",
    "#### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 400\n",
    "\n",
    "import numpy.random as npr\n",
    "\n",
    "\n",
    "def batches(training_data: Array, batch_size: int) -> Generator:\n",
    "    num_train = training_data.shape[0]\n",
    "    num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "    num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "    # batching mechanism, ripped from the JAX docs :)\n",
    "    def data_stream():\n",
    "        rng = npr.RandomState(0)\n",
    "        while True:\n",
    "            perm = rng.permutation(num_train)\n",
    "            for i in range(num_batches):\n",
    "                batch_idx = perm[i * batch_size : (i + 1) * batch_size]\n",
    "                yield train[batch_idx], train_labels[batch_idx]\n",
    "\n",
    "    return data_stream()\n",
    "\n",
    "\n",
    "batch = batches(train, batch_size)\n",
    "one_hot_valid, one_hot_valid_labels = batch_one_hot(valid), batch_one_hot(valid_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparams, modify at will!\n",
    "num_iter = 2000\n",
    "lr = 4e-2\n",
    "best_loss = 999\n",
    "best_pars = None\n",
    "\n",
    "\n",
    "# basic gradient descent\n",
    "def gradient_descent(param: jax.Array, grads: jax.Array) -> jax.Array:\n",
    "    return param - lr * grads.mean(axis=0)\n",
    "\n",
    "\n",
    "# more advanced gradient descent\n",
    "import optax\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip(1),\n",
    "    optax.adamw(learning_rate=lr),\n",
    ")\n",
    "opt_state = opt.init(pars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_iter):\n",
    "    sentences, sentence_labels = next(batch)\n",
    "    one_hot_sentences, one_hot_sentence_labels = batch_one_hot(\n",
    "        sentences, vocab_size=v\n",
    "    ), batch_one_hot(sentence_labels, vocab_size=v)\n",
    "    loss, grads = batched_grads(one_hot_sentences, one_hot_sentence_labels, pars, h)\n",
    "    valid_loss, _ = batched_grads(one_hot_valid, one_hot_valid_labels, pars, h)\n",
    "    loss, valid_loss = loss.mean(), valid_loss.mean()\n",
    "\n",
    "    # gradient descent!\n",
    "    pars = jax.tree_map(gradient_descent, pars, grads)\n",
    "\n",
    "    ## uncomment these lines for advanced version\n",
    "    # avg_grads = jax.tree_map(lambda g: g.mean(axis=0), grads)\n",
    "    # updates, opt_state = opt.update(avg_grads, opt_state, params=pars)\n",
    "    # pars = optax.apply_updates(pars, updates)\n",
    "\n",
    "    if valid_loss < best_loss:\n",
    "        best_pars = deepcopy(pars)\n",
    "        best_loss = valid_loss\n",
    "    if i % 20 == 0:\n",
    "        print(f\"train loss: {loss.mean():.3f}\", end=\", \")\n",
    "        print(f\"valid loss: {valid_loss.mean():.3f}\")\n",
    "\n",
    "print(f\"best valid loss: {best_loss:.3f}\")\n",
    "print(predict_next_words(\"Red fish \", vocab, pars, h, 10, include_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
