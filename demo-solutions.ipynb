{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install equinox jaxtyping jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import re\n",
    "from jaxtyping import Array, Float, Int\n",
    "import pytreeclass as pytc\n",
    "from typing import Generator\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def prepare_text(file_name, sentence_length):\n",
    "    with open(file_name, \"r+\") as file:\n",
    "        all_text = file.read()\n",
    "        # all_text = all_text.replace('\\n', ' ').replace('  : ', '')\n",
    "\n",
    "    # Define a regular expression pattern to match all punctuation marks\n",
    "    punctuation_pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "    # Define a regular expression pattern to match words with apostrophes\n",
    "    apostrophe_pattern = r\"\\w+(?:\\'\\w+)?\"\n",
    "    # Define a regular expression pattern to match newlines\n",
    "    newline_pattern = r\"\\n\"\n",
    "\n",
    "    # Combine the three patterns to match all tokens\n",
    "    token_pattern = (\n",
    "        punctuation_pattern + \"|\" + apostrophe_pattern + \"|\" + newline_pattern\n",
    "    )\n",
    "\n",
    "    # Split the text into tokens, including words with apostrophes as separate tokens\n",
    "    all_words = re.findall(token_pattern, all_text.lower())\n",
    "    vocab = list(set(all_words))\n",
    "\n",
    "    vocab_one_hot_indicies = jnp.array(\n",
    "        [vocab.index(t) for t in all_words], dtype=jnp.int32\n",
    "    )\n",
    "    split_indicies = vocab_one_hot_indicies[\n",
    "        : (len(vocab) // sentence_length) * sentence_length\n",
    "    ].reshape(len(vocab) // sentence_length, sentence_length)\n",
    "    # make last word random, shouldn't make too much of an impact (could be better handled with special char?)\n",
    "    split_indicies_labels = jnp.concatenate(\n",
    "        (\n",
    "            vocab_one_hot_indicies[\n",
    "                1 : ((len(vocab) - 1) // sentence_length) * sentence_length\n",
    "            ],\n",
    "            jnp.array([0]),\n",
    "        )\n",
    "    ).reshape((len(vocab) - 1) // sentence_length, sentence_length)\n",
    "    partition_index = 6 * int(len(split_indicies) / 7)\n",
    "    train = split_indicies[:partition_index]\n",
    "    train_labels = split_indicies_labels[:partition_index]\n",
    "    valid = split_indicies[partition_index:]\n",
    "    valid_labels = split_indicies_labels[partition_index:]\n",
    "\n",
    "    return train, train_labels, valid, valid_labels, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples from vocab: ['thin', 'brush', 'one', 'may', 'called', 'sad', 'look', 'but', 'high', 'get']\n",
      "total length of vocab: 300 unique words\n",
      "total length of training data: 30 sentences (each 8 words)\n",
      "total length of validation data: 7 sentences (each 8 words)\n"
     ]
    }
   ],
   "source": [
    "file_name = \"one-fish-two-fish.txt\"\n",
    "sentence_length = 8  # keep even because of how we split the data\n",
    "train, train_labels, valid, valid_labels, vocab = prepare_text(\n",
    "    file_name, sentence_length\n",
    ")\n",
    "\n",
    "print(f\"examples from vocab: {vocab[:10]}\")\n",
    "print(f\"total length of vocab: {len(vocab)} unique words\")\n",
    "print(\n",
    "    f\"total length of training data: {len(train)} sentences (each {sentence_length} words)\"\n",
    ")\n",
    "print(\n",
    "    f\"total length of validation data: {len(valid)} sentences (each {sentence_length} words)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([  2, 186, 233, 227, 186, 233,  83, 186], dtype=int32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first sentence in train set\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([186, 233, 227, 186, 233,  83, 186, 233], dtype=int32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first sentence in train labels == same sentence shifted by one word\n",
    "# i.e. equivalent to train[0][1:] + train[1][0]\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one fish , two fish , red fish'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can reconstruct a sentence by mapping indicies back to words\n",
    "\" \".join([vocab[i] for i in train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fish , two fish , red fish ,'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([vocab[i] for i in train_labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish_idx = vocab.index(\"fish\")  # the index of the word \"fish\" in the vocab\n",
    "one_hot_fish = np.zeros(len(vocab))  # a vector of zeros with length equal to the vocab\n",
    "one_hot_fish[fish_idx] = 1  # set the index of the word \"fish\" to 1\n",
    "\n",
    "# the syntax in JAX is a little different, but the idea is the same\n",
    "# we use the `at` method to set the value at a particular index,\n",
    "# and the `set` method to set the value at that index to 1.\n",
    "# this is due to the fact that JAX arrays are immutable,\n",
    "# so we can't just set the value at an index to 1 directly!\n",
    "fish_idx = vocab.index(\"fish\")\n",
    "one_hot_fish = jnp.zeros(len(vocab))\n",
    "one_hot_fish = one_hot_fish.at[fish_idx].set(1)\n",
    "\n",
    "one_hot_fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_sentence(\n",
    "    sentence: Int[Array, \"sentence\"], vocab_size: int\n",
    ") -> Int[Array, \"sentence vocab\"]:\n",
    "    return jnp.array([jnp.zeros((vocab_size,)).at[word].set(1) for word in sentence])\n",
    "\n",
    "\n",
    "# make a very intelligent sentence of all \"fish\"\n",
    "fish_sentence = jnp.array([fish_idx] * 10)\n",
    "\n",
    "# one hot encode the sentence\n",
    "one_hot_fish_sentence = one_hot_sentence(fish_sentence, len(vocab))\n",
    "\n",
    "# assert that the sentence is one hot encoded correctly\n",
    "assert jnp.all(one_hot_fish_sentence == jnp.array([one_hot_fish] * 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use `vmap` to automatically transform the function to work on a batch of sentences!\n",
    "# this will be useful when we want to train our model on multiple sentences at once.\n",
    "# note that we need to specify the `in_axes` argument to tell JAX which argument\n",
    "# in the function is the one that we want to map over (in this case, we want to\n",
    "# map over the first axis of the `sentence` argument, indicated by `0`).\n",
    "# we also need to specify `None` for the `vocab_size` argument, since it is not\n",
    "# being mapped over -- it is the same for every sentence in the batch.\n",
    "batch_one_hot = jax.vmap(one_hot_sentence, in_axes=(0, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────────────┬───────────┬──────┐\n",
      "│Name                 │Type       │Count │\n",
      "├─────────────────────┼───────────┼──────┤\n",
      "│.embedding_weights   │f32[16,30] │480   │\n",
      "├─────────────────────┼───────────┼──────┤\n",
      "│.hidden_state_weights│f32[16,16] │256   │\n",
      "├─────────────────────┼───────────┼──────┤\n",
      "│.output_weights      │f32[300,16]│4,800 │\n",
      "├─────────────────────┼───────────┼──────┤\n",
      "│.hidden_state_bias   │f32[16]    │16    │\n",
      "├─────────────────────┼───────────┼──────┤\n",
      "│.output_bias         │f32[300]   │300   │\n",
      "├─────────────────────┼───────────┼──────┤\n",
      "│.embedding_matrix    │f32[30,300]│9,000 │\n",
      "├─────────────────────┼───────────┼──────┤\n",
      "│Σ                    │Parameters │14,852│\n",
      "└─────────────────────┴───────────┴──────┘\n",
      "Parameters\n",
      "├── .embedding_weights=f32[16,30](μ=-0.00, σ=0.06, ∈[-0.10,0.10])\n",
      "├── .hidden_state_weights=f32[16,16](μ=0.06, σ=0.24, ∈[0.00,1.00])\n",
      "├── .output_weights=f32[300,16](μ=-0.00, σ=0.06, ∈[-0.10,0.10])\n",
      "├── .hidden_state_bias=f32[16](μ=0.00, σ=0.00, ∈[0.00,0.00])\n",
      "├── .output_bias=f32[300](μ=0.00, σ=0.00, ∈[0.00,0.00])\n",
      "└── .embedding_matrix=f32[30,300](μ=-0.00, σ=0.06, ∈[-0.10,0.10])\n"
     ]
    }
   ],
   "source": [
    "# This is a container for all the free parameters of an RNN!\n",
    "# You can see the shapes of each attribute from the type annotations.\n",
    "# We have a couple of sizes: hidden_state, embedding, vocab\n",
    "# -> these represent the size of the hidden state weights,\n",
    "#    the embedding matrix, and the vocabulary respectively.\n",
    "# This parameters object will be passed to most functions below:\n",
    "# e.g. access the output weights by calling `params.output_weights` etc.\n",
    "class Parameters(pytc.TreeClass):\n",
    "    embedding_weights: Float[Array, \"hidden_state embedding\"]\n",
    "    hidden_state_weights: Float[Array, \"hidden_state hidden_state\"]\n",
    "    output_weights: Float[Array, \"vocab hidden_state\"]\n",
    "    hidden_state_bias: Float[Array, \"hidden_state\"]\n",
    "    output_bias: Float[Array, \"vocab\"]\n",
    "    embedding_matrix: Float[Array, \"embedding vocab\"]\n",
    "\n",
    "\n",
    "# we'll initialize our parameters randomly, but close to 0/identity so that\n",
    "# we don't have exploding gradients later on!\n",
    "\n",
    "# set sizes for embeddings, hidden state, vocab, and output vectors\n",
    "e = 30\n",
    "h = 16\n",
    "v = len(vocab)\n",
    "o = v\n",
    "\n",
    "params = Parameters(\n",
    "    embedding_weights=jax.random.truncated_normal(\n",
    "        lower=-0.1, upper=0.1, shape=[h, e], key=jax.random.PRNGKey(0)\n",
    "    ),\n",
    "    hidden_state_weights=jnp.identity(h),\n",
    "    output_weights=jax.random.truncated_normal(\n",
    "        lower=-0.1, upper=0.1, shape=[o, h], key=jax.random.PRNGKey(0)\n",
    "    ),\n",
    "    hidden_state_bias=jnp.zeros((h,)),\n",
    "    output_bias=jnp.zeros(\n",
    "        shape=[\n",
    "            o,\n",
    "        ]\n",
    "    ),\n",
    "    embedding_matrix=jax.random.truncated_normal(\n",
    "        lower=-0.1, upper=0.1, shape=[e, v], key=jax.random.PRNGKey(0)\n",
    "    ),\n",
    ")\n",
    "\n",
    "# let's inspect the structure of our parameters\n",
    "print(pytc.tree_summary(params))\n",
    "print(pytc.tree_diagram(params))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll come to use most of these values later, but for now, we're just focused on embeddings!\n",
    "\n",
    "Recall what we did in the previous sessions, looking at RNNs for *language modelling*, where every word is turned into a one-hot vector (or \"token\"). We then multiply these one-hot words by an *embedding matrix* $E$, which multiplies the words to reduce the dimension of that long one-hot vector (=size of the whole vocabulary) to some specified lower dimensional representation (normally ~100 ish). This embedded word is then used to update the hidden state $h$ of the RNN.\n",
    "\n",
    "Use the embedding matrix (accessible through `params.embedding_matrix`) to fill in the function below, which embeds a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings(\n",
    "    one_hot_word: Float[Array, \"vocab\"], params: Parameters\n",
    ") -> Float[Array, \"embedding\"]:\n",
    "    return params.embedding_matrix @ one_hot_word\n",
    "\n",
    "\n",
    "# I should be a vector of length `e`\n",
    "assert make_embeddings(one_hot_fish, params).shape == (e,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map to work over sentences for later!\n",
    "embeddings_map = jax.vmap(make_embeddings, in_axes=(0, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_hidden_state(\n",
    "    embedding: Float[Array, \"embedding\"],\n",
    "    hidden_state: Float[Array, \"hidden_state\"],\n",
    "    params: Parameters,\n",
    ") -> Float[Array, \"hidden_state\"]:\n",
    "    return jax.nn.tanh(\n",
    "        params.hidden_state_weights @ hidden_state\n",
    "        + params.embedding_weights @ embedding\n",
    "        + params.hidden_state_bias\n",
    "    )\n",
    "\n",
    "\n",
    "# I should be a vector of length `h`\n",
    "embedding = make_embeddings(one_hot_fish, params)\n",
    "assert update_hidden_state(embedding, jnp.zeros((h,)), params).shape == (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(\n",
    "    hidden_state: Float[Array, \"hidden_state\"], params: Parameters\n",
    ") -> Float[Array, \"vocab\"]:\n",
    "    return jax.nn.softmax(params.output_weights @ hidden_state + params.output_bias)\n",
    "\n",
    "\n",
    "# I should be a vector of length `v`\n",
    "assert output(jnp.zeros((h,)), params).shape == (v,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['then', 'then', 'then', 'then', 'then', 'then', 'then', 'then', 'sun', 'sun']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rnn(\n",
    "    data: Float[Array, \"sentence vocab\"], params: Parameters, hidden_size: int\n",
    ") -> Float[Array, \"sentence vocab\"]:\n",
    "    # apply embeddings_map to create a vector of embeddings\n",
    "    embeddings = embeddings_map(data, params)  # [\"sentence embedding\"]\n",
    "\n",
    "    # initialize the hidden state with zeros\n",
    "    hidden_state = jnp.zeros((hidden_size,))\n",
    "\n",
    "    # for each word in the vector of embeddings:\n",
    "    #   > update the hidden state\n",
    "    #   > compute the output word using that hidden state and store it\n",
    "    # return the set of outputs\n",
    "    outputs = []\n",
    "\n",
    "    for word in embeddings:\n",
    "        hidden_state = update_hidden_state(word, hidden_state, params)\n",
    "        outputs.append(output(hidden_state, params))\n",
    "\n",
    "    return jnp.array(outputs)\n",
    "\n",
    "\n",
    "# make a very intelligent sentence of all \"fish\"\n",
    "fish_sentence = jnp.array([fish_idx] * 10)\n",
    "one_hot_fish_sentence = one_hot_sentence(fish_sentence, len(vocab))\n",
    "\n",
    "# run the RNN on \"fish fish fish fish fish fish fish fish fish fish\"\n",
    "rnn_outputs = rnn(one_hot_fish_sentence, params, h)\n",
    "\n",
    "# the output should be a list of 10 vectors of length `v`\n",
    "# corresponding to the output probabilities at each position in the sentence\n",
    "assert rnn_outputs.shape == (10, v)\n",
    "\n",
    "# what is the most likely word at each position?\n",
    "# (remember that we have randomly initialized our parameters, so this will be nonsense!)\n",
    "most_likely_words = [vocab[jnp.argmax(output)] for output in rnn_outputs]\n",
    "most_likely_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(\n",
    "    output: Float[Array, \"vocab\"], next_one_hot_word: Float[Array, \"vocab\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    # index the softmax probs at the word of interest\n",
    "    return -jnp.log(output[jnp.argmax(next_one_hot_word)])\n",
    "\n",
    "\n",
    "sentence_loss = jax.vmap(loss, in_axes=(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loss: 5.69\n",
      "gradients are packed up in a Parameters object:\n",
      "Parameters\n",
      "├── .embedding_weights=f32[16,30](μ=-0.00, σ=0.02, ∈[-0.05,0.05])\n",
      "├── .hidden_state_weights=f32[16,16](μ=0.00, σ=0.02, ∈[-0.04,0.06])\n",
      "├── .output_weights=f32[300,16](μ=-0.00, σ=0.01, ∈[-0.15,0.20])\n",
      "├── .hidden_state_bias=f32[16](μ=-0.11, σ=0.28, ∈[-0.50,0.32])\n",
      "├── .output_bias=f32[300](μ=0.00, σ=0.06, ∈[-1.00,0.00])\n",
      "└── .embedding_matrix=f32[30,300](μ=-0.00, σ=0.00, ∈[-0.15,0.07])\n"
     ]
    }
   ],
   "source": [
    "def forward_pass(\n",
    "    data: Float[Array, \"sentence vocab\"],\n",
    "    next_words: Float[Array, \"sentence vocab\"],  # data shifted by 1 to the right\n",
    "    params: Parameters,\n",
    "    hidden_size: int,\n",
    ") -> Float[Array, \"\"]:\n",
    "    output = rnn(data, params, hidden_size)\n",
    "    return sentence_loss(output, next_words).mean(axis=0)\n",
    "\n",
    "\n",
    "# run the forward pass on our fish sentence\n",
    "loss_value = forward_pass(one_hot_fish_sentence, one_hot_fish_sentence, params, h)\n",
    "assert loss_value.shape == ()\n",
    "print(f\"starting loss: {loss_value:.2f}\")\n",
    "\n",
    "# here, we transform the forward pass into the gradient function,\n",
    "# and also vmap again so it can handle a batch of sentences instead of one.\n",
    "loss_and_gradient = jax.value_and_grad(forward_pass, argnums=2)\n",
    "print(\"gradients are packed up in a Parameters object:\")\n",
    "print(\n",
    "    pytc.tree_diagram(\n",
    "        loss_and_gradient(one_hot_fish_sentence, one_hot_fish_sentence, params, h)[1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also vmap the gradient function to handle a batch of sentences,\n",
    "# and jit it to make it faster!\n",
    "batched_grads = jax.jit(\n",
    "    jax.vmap(loss_and_gradient, in_axes=(0, 0, None, None)), static_argnums=(3,)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_words(\n",
    "    prompt: str,\n",
    "    vocab: list[str],\n",
    "    rnn_params: Parameters,\n",
    "    rnn_hidden_size: int,\n",
    "    num_predicted_tokens: int,\n",
    "    include_prompt=True,\n",
    ") -> str:\n",
    "    # Define a regular expression pattern to match all punctuation marks\n",
    "    punctuation_pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "    # Define a regular expression pattern to match words with apostrophes\n",
    "    apostrophe_pattern = r\"\\w+(?:\\'\\w+)?\"\n",
    "    # Define a regular expression pattern to match newlines\n",
    "    newline_pattern = r\"\\n\"\n",
    "\n",
    "    # Combine the three patterns to match all tokens\n",
    "    token_pattern = (\n",
    "        punctuation_pattern + \"|\" + apostrophe_pattern + \"|\" + newline_pattern\n",
    "    )\n",
    "\n",
    "    tokens = re.findall(token_pattern, prompt.lower())\n",
    "    one_hot_indicies = jnp.array([vocab.index(t) for t in tokens], dtype=jnp.int32)\n",
    "    sentence = one_hot_sentence(one_hot_indicies, len(vocab))\n",
    "    embeddings = embeddings_map(sentence, rnn_params)  # [\"sentence embedding\"]\n",
    "\n",
    "    hidden_state = jnp.zeros((rnn_hidden_size,))\n",
    "    outputs = [None] * num_predicted_tokens\n",
    "    for word in embeddings[:-1]:\n",
    "        hidden_state = update_hidden_state(word, hidden_state, rnn_params)\n",
    "    hidden_state = update_hidden_state(embeddings[-1], hidden_state, rnn_params)\n",
    "    outputs[0] = output(hidden_state, rnn_params)\n",
    "\n",
    "    for i in range(1, num_predicted_tokens):\n",
    "        embedded_pred = make_embeddings(outputs[i - 1], rnn_params)\n",
    "        hidden_state = update_hidden_state(embedded_pred, hidden_state, rnn_params)\n",
    "        outputs[i] = output(hidden_state, rnn_params)\n",
    "\n",
    "    res = jnp.array(outputs)\n",
    "    res_indicies = jnp.argmax(res, axis=1)\n",
    "    words = [vocab[i] for i in res_indicies]\n",
    "    out = \" \".join(words)\n",
    "    return prompt + \" | \" + out if include_prompt else out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 400\n",
    "\n",
    "import numpy.random as npr\n",
    "\n",
    "\n",
    "def batches(training_data: Array, batch_size: int) -> Generator:\n",
    "    num_train = training_data.shape[0]\n",
    "    num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "    num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "    # batching mechanism, ripped from the JAX docs :)\n",
    "    def data_stream():\n",
    "        rng = npr.RandomState(0)\n",
    "        while True:\n",
    "            perm = rng.permutation(num_train)\n",
    "            for i in range(num_batches):\n",
    "                batch_idx = perm[i * batch_size : (i + 1) * batch_size]\n",
    "                yield train[batch_idx], train_labels[batch_idx]\n",
    "\n",
    "    return data_stream()\n",
    "\n",
    "\n",
    "batch = batches(train, batch_size)\n",
    "one_hot_valid, one_hot_valid_labels = batch_one_hot(valid, len(vocab)), batch_one_hot(\n",
    "    valid_labels, len(vocab)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparams, modify at will!\n",
    "num_iter = 2000\n",
    "lr = 4e-2\n",
    "best_loss = 999\n",
    "best_pars = None\n",
    "\n",
    "\n",
    "# basic gradient descent\n",
    "def gradient_descent(param: jax.Array, grads: jax.Array) -> jax.Array:\n",
    "    return param - lr * grads.mean(axis=0)\n",
    "\n",
    "\n",
    "# more advanced gradient descent\n",
    "import optax\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip(1),\n",
    "    optax.adamw(learning_rate=lr),\n",
    ")\n",
    "opt_state = opt.init(params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 5.704, valid loss: 5.705\n",
      "train loss: 5.638, valid loss: 5.633\n",
      "train loss: 5.542, valid loss: 5.532\n",
      "train loss: 5.401, valid loss: 5.391\n",
      "train loss: 5.223, valid loss: 5.219\n",
      "train loss: 5.040, valid loss: 5.048\n",
      "train loss: 4.881, valid loss: 4.910\n",
      "train loss: 4.757, valid loss: 4.816\n",
      "train loss: 4.660, valid loss: 4.757\n",
      "train loss: 4.585, valid loss: 4.721\n",
      "train loss: 4.525, valid loss: 4.700\n",
      "train loss: 4.476, valid loss: 4.689\n",
      "train loss: 4.435, valid loss: 4.682\n",
      "train loss: 4.400, valid loss: 4.679\n",
      "train loss: 4.369, valid loss: 4.678\n",
      "train loss: 4.341, valid loss: 4.678\n",
      "train loss: 4.316, valid loss: 4.678\n",
      "train loss: 4.293, valid loss: 4.679\n",
      "train loss: 4.271, valid loss: 4.680\n",
      "train loss: 4.252, valid loss: 4.681\n",
      "train loss: 4.234, valid loss: 4.682\n",
      "train loss: 4.218, valid loss: 4.684\n",
      "train loss: 4.202, valid loss: 4.685\n",
      "train loss: 4.188, valid loss: 4.687\n",
      "train loss: 4.175, valid loss: 4.688\n",
      "train loss: 4.162, valid loss: 4.690\n",
      "train loss: 4.151, valid loss: 4.692\n",
      "train loss: 4.140, valid loss: 4.694\n",
      "train loss: 4.129, valid loss: 4.696\n",
      "train loss: 4.120, valid loss: 4.698\n",
      "train loss: 4.111, valid loss: 4.700\n",
      "train loss: 4.102, valid loss: 4.702\n",
      "train loss: 4.094, valid loss: 4.704\n",
      "train loss: 4.086, valid loss: 4.706\n",
      "train loss: 4.079, valid loss: 4.708\n",
      "train loss: 4.072, valid loss: 4.710\n",
      "train loss: 4.065, valid loss: 4.712\n",
      "train loss: 4.059, valid loss: 4.714\n",
      "train loss: 4.053, valid loss: 4.716\n",
      "train loss: 4.048, valid loss: 4.718\n",
      "train loss: 4.042, valid loss: 4.719\n",
      "train loss: 4.037, valid loss: 4.721\n",
      "train loss: 4.032, valid loss: 4.723\n",
      "train loss: 4.027, valid loss: 4.725\n",
      "train loss: 4.023, valid loss: 4.727\n",
      "train loss: 4.018, valid loss: 4.729\n",
      "train loss: 4.014, valid loss: 4.731\n",
      "train loss: 4.010, valid loss: 4.733\n",
      "train loss: 4.006, valid loss: 4.735\n",
      "train loss: 4.003, valid loss: 4.737\n",
      "train loss: 3.999, valid loss: 4.738\n",
      "train loss: 3.996, valid loss: 4.740\n",
      "train loss: 3.992, valid loss: 4.742\n",
      "train loss: 3.989, valid loss: 4.744\n",
      "train loss: 3.986, valid loss: 4.746\n",
      "train loss: 3.983, valid loss: 4.747\n",
      "train loss: 3.980, valid loss: 4.749\n",
      "train loss: 3.977, valid loss: 4.751\n",
      "train loss: 3.975, valid loss: 4.753\n",
      "train loss: 3.972, valid loss: 4.755\n",
      "train loss: 3.970, valid loss: 4.756\n",
      "train loss: 3.967, valid loss: 4.758\n",
      "train loss: 3.965, valid loss: 4.760\n",
      "train loss: 3.962, valid loss: 4.762\n",
      "train loss: 3.960, valid loss: 4.763\n",
      "train loss: 3.958, valid loss: 4.765\n",
      "train loss: 3.956, valid loss: 4.767\n",
      "train loss: 3.954, valid loss: 4.769\n",
      "train loss: 3.952, valid loss: 4.770\n",
      "train loss: 3.950, valid loss: 4.772\n",
      "train loss: 3.948, valid loss: 4.774\n",
      "train loss: 3.946, valid loss: 4.775\n",
      "train loss: 3.944, valid loss: 4.777\n",
      "train loss: 3.943, valid loss: 4.779\n",
      "train loss: 3.941, valid loss: 4.781\n",
      "train loss: 3.939, valid loss: 4.782\n",
      "train loss: 3.937, valid loss: 4.784\n",
      "train loss: 3.936, valid loss: 4.786\n",
      "train loss: 3.934, valid loss: 4.788\n",
      "train loss: 3.933, valid loss: 4.789\n",
      "train loss: 3.931, valid loss: 4.791\n",
      "train loss: 3.930, valid loss: 4.793\n",
      "train loss: 3.928, valid loss: 4.794\n",
      "train loss: 3.927, valid loss: 4.796\n",
      "train loss: 3.926, valid loss: 4.798\n",
      "train loss: 3.924, valid loss: 4.800\n",
      "train loss: 3.923, valid loss: 4.801\n",
      "train loss: 3.922, valid loss: 4.803\n",
      "train loss: 3.920, valid loss: 4.805\n",
      "train loss: 3.919, valid loss: 4.806\n",
      "train loss: 3.918, valid loss: 4.808\n",
      "train loss: 3.917, valid loss: 4.810\n",
      "train loss: 3.916, valid loss: 4.811\n",
      "train loss: 3.914, valid loss: 4.813\n",
      "train loss: 3.913, valid loss: 4.815\n",
      "train loss: 3.912, valid loss: 4.816\n",
      "train loss: 3.911, valid loss: 4.818\n",
      "train loss: 3.910, valid loss: 4.820\n",
      "train loss: 3.909, valid loss: 4.821\n",
      "train loss: 3.908, valid loss: 4.823\n",
      "best valid loss: 4.678\n",
      "Red fish  | \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = params\n",
    "for i in range(num_iter):\n",
    "    sentences, sentence_labels = next(batch)\n",
    "    one_hot_sentences, one_hot_sentence_labels = batch_one_hot(\n",
    "        sentences, v\n",
    "    ), batch_one_hot(sentence_labels, v)\n",
    "    loss, grads = batched_grads(one_hot_sentences, one_hot_sentence_labels, pars, h)\n",
    "    valid_loss, _ = batched_grads(one_hot_valid, one_hot_valid_labels, pars, h)\n",
    "    loss, valid_loss = loss.mean(), valid_loss.mean()\n",
    "\n",
    "    # gradient descent!\n",
    "    pars = jax.tree_map(gradient_descent, pars, grads)\n",
    "\n",
    "    ## uncomment these lines for advanced version\n",
    "    # avg_grads = jax.tree_map(lambda g: g.mean(axis=0), grads)\n",
    "    # updates, opt_state = opt.update(avg_grads, opt_state, params=pars)\n",
    "    # pars = optax.apply_updates(pars, updates)\n",
    "\n",
    "    if valid_loss < best_loss:\n",
    "        best_pars = deepcopy(pars)\n",
    "        best_loss = valid_loss\n",
    "    if i % 20 == 0:\n",
    "        print(f\"train loss: {loss.mean():.3f}\", end=\", \")\n",
    "        print(f\"valid loss: {valid_loss.mean():.3f}\")\n",
    "\n",
    "print(f\"best valid loss: {best_loss:.3f}\")\n",
    "print(predict_next_words(\"Red fish \", vocab, pars, h, 10, include_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red fish  | \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(predict_next_words(\"Red fish \", vocab, pars, h, 10, include_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
